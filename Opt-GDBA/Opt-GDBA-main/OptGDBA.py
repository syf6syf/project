import sys, os
sys.path.append(os.path.abspath('..'))

import numpy as np
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import copy
from mask import gen_mask
from input import gen_input
from main import init_trigger

from sklearn.cluster import KMeans

import networkx as nx
import random
import hashlib

#Algorithm 1 ğ‘˜-means_gap to learn customized trigger size
#å‡½æ•°detection1ä¸detectionä¸ºæ­¤å…¬å¼å¤ç°ã€‚
#å…¶ä¸­detection1()ä¸ºç¡®å®šæœ€ä½³èšç±»Kå€¼,detection()è¿›è¡Œæœ€ç»ˆèšç±»å¾—åˆ°è§¦å‘å™¨å¤§å°ã€‚

# ä½¿ç”¨ KMeans èšç±» å¯¹å¾—åˆ†è¿›è¡Œèšç±»ï¼Œæ® gap å€¼çš„å˜åŒ–æ¥é€‰å–åˆé€‚çš„èšç±»æ•° k
def detection1(score):
    score = score.numpy() # score æ˜¯ä¸€ä¸ªè¡¨ç¤ºå›¾æˆ–èŠ‚ç‚¹ç‰¹å¾çš„å¾—åˆ†æ•°ç»„
    nrefs = 10 # è®¾ç½®å‚è€ƒèšç±»çš„æ•°é‡ã€‚è¿™ä¸ªå‚æ•°ç”¨äºç”Ÿæˆå‚è€ƒèšç±»ç»“æœ
    ks = range(1, 8)
    if len(score) < 8:
        ks = range(1, len(score)) # å¯èƒ½çš„èšç±»æ•° k çš„èŒƒå›´ï¼Œè¦å°è¯•çš„ä¸åŒç°‡æ•°
    gaps = np.zeros(len(ks)) # å­˜å‚¨ä¸åŒèšç±»æ•°ä¸‹çš„ gap å€¼ å’Œ gap å€¼å˜åŒ–
    gapDiff = np.zeros(len(ks) - 1)
    sdk = np.zeros(len(ks)) # æ ‡å‡†å·®
    min = np.min(score) # min å’Œ max ç”¨äºå¯¹å¾—åˆ†è¿›è¡Œ å½’ä¸€åŒ–
    max = np.max(score)
    score = (score - min)/(max-min) # å½’ä¸€åŒ–æ“ä½œ
    for i, k in enumerate(ks): # å¯¹æ¯ä¸ªå¯èƒ½çš„ k å€¼ï¼Œä½¿ç”¨ KMeans èšç±»ç®—æ³•å¯¹ score è¿›è¡Œèšç±»
        estimator = KMeans(n_clusters=k) # åˆå§‹åŒ– KMeans èšç±»å™¨ï¼ŒæŒ‡å®šåˆ†æˆkä¸ªç°‡
        estimator.fit(score.reshape(-1, 1)) # fit() æ˜¯ KMeans çš„è®­ç»ƒå‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªæ•°æ®é›†ï¼ˆç‰¹å¾çŸ©é˜µï¼‰ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†æ•°æ®è¿›è¡Œèšç±»ï¼ˆå°† score è½¬æ¢ä¸ºåˆ—å‘é‡è¾“å…¥ï¼‰ã€‚
        label_pred = estimator.labels_
        center = estimator.cluster_centers_
        Wk = np.sum([np.square(score[m]-center[label_pred[m]]) for m in range(len(score))]) # è®¡ç®—äº†æ¯ä¸ªæ•°æ®ç‚¹ä¸å…¶æ‰€å±ç°‡ä¸­å¿ƒçš„ å¹³æ–¹è·ç¦»ï¼Œå¹¶å°†è¿™äº›å¹³æ–¹è·ç¦»åŠ æ€»ï¼Œå¾—åˆ°ä¸€ä¸ª ç°‡å†…ç¦»æ•£åº¦

        WkRef = np.zeros(nrefs)
        for j in range(nrefs): #å¯¹äºæ¯ä¸ªèšç±»æ•° kï¼Œç”Ÿæˆ nrefs ä¸ªå‚è€ƒèšç±»ç»“æœï¼Œè¿™äº›ç»“æœé€šè¿‡åœ¨ [0, 1] åŒºé—´ç”Ÿæˆéšæœºæ•°æ®æ¥æ¨¡æ‹Ÿä¸çœŸå®æ•°æ®ä¸åŒçš„èšç±»æ•ˆæœã€‚
            rand = np.random.uniform(0, 1, len(score))
            estimator = KMeans(n_clusters=k)
            estimator.fit(rand.reshape(-1, 1))
            label_pred = estimator.labels_
            center = estimator.cluster_centers_
            WkRef[j] = np.sum([np.square(rand[m]-center[label_pred[m]]) for m in range(len(rand))]) # å¯¹å‚è€ƒèšç±»è¿›è¡Œç›¸åŒçš„èšç±»æ“ä½œï¼Œè®¡ç®—å‚è€ƒèšç±»çš„ç¦»æ•£åº¦ WkRef
        gaps[i] = np.log(np.mean(WkRef)) - np.log(Wk)  # è®¡ç®—æ¯ä¸ª k å¯¹åº”çš„ gap å€¼ï¼Œgap å€¼è¶Šå¤§ï¼Œè¡¨ç¤ºèšç±»æ•ˆæœè¶Šå¥½ã€‚
        sdk[i] = np.sqrt((1.0 + nrefs) / nrefs) * np.std(np.log(WkRef)) #  gap å€¼çš„æ ‡å‡†å·®ï¼Œç”¨æ¥é‡åŒ– gap å€¼çš„ä¸ç¡®å®šæ€§

        if i > 0:
            gapDiff[i - 1] = gaps[i - 1] - gaps[i] + sdk[i] # ç”¨äºè¡¡é‡å½“å‰ k å’Œå‰ä¸€ä¸ª k ä¹‹é—´çš„ gap å˜åŒ–ï¼Œå¦‚æœå˜åŒ–è¾ƒå¤§ï¼Œè¯´æ˜èšç±»æ•° k å¯èƒ½æ›´åˆé€‚ã€‚
    #print(gapDiff)
    select_k = 3
    for i in range(len(gapDiff)):
        if gapDiff[i] >= 0: # å€¼æ˜¯å·®ï¼Œå€¼ä¸ºæ­£è¡¨ç¤ºåè€…æ›´å¤§
            select_k = i+1 # è¯´æ˜å½“å‰ k çš„èšç±»æ•ˆæœæ›´å¥½ï¼Œé€‰æ‹©å½“å‰ k ä¸ºæœ€ä½³èšç±»æ•°
            break
    return select_k

#è¿”å›æœ€ç»ˆè§¦å‘å™¨å¤§å°
#å¯¹ score æ•°æ®è¿›è¡Œèšç±»ã€‚
#æ‰¾å‡ºèšç±»ç»“æœä¸­å¹³å‡å€¼æœ€å¤§çš„ç°‡ï¼Œå¹¶è¿”å›è¯¥ç°‡ä¸­æ•°æ®ç‚¹çš„æ•°é‡ã€‚
def detection(score, k_value): # æ ¹æ®ç»™å®šçš„ k_valueï¼Œä½¿ç”¨ KMeans èšç±»ç®—æ³•å¯¹å¾—åˆ†è¿›è¡Œèšç±»ï¼Œè¾“å‡ºæœ€å¤§ç°‡çš„èŠ‚ç‚¹æ•°
    score = score.numpy()
    estimator = KMeans(n_clusters=k_value)
    estimator.fit(score.reshape(-1, 1))
    label_pred = estimator.labels_
    trigger_size = {}
    temp_max = 0
    temp_size = 0
    for i in range(k_value): # è®¡ç®—æ¯ä¸ªç°‡çš„å¹³å‡å¾—åˆ†
        trigger_size[i] = np.mean(score[label_pred==i])

    for i in range(k_value): # æ‰¾å‡ºæœ€å¤§å¹³å‡å¾—åˆ†å¯¹åº”çš„ç°‡çš„èŠ‚ç‚¹æ•°é‡
        if trigger_size[i] > temp_max:
            temp_max = trigger_size[i]
            temp_size = len(label_pred==i)
    return  int(temp_size)       
    
#é€‰æ‹©åˆé€‚çš„èŠ‚ç‚¹ä½œä¸ºè§¦å‘å™¨çš„ä½ç½®ï¼Œè§¦å‘å™¨çš„ä½ç½®å†³å®šäº†å“ªäº›èŠ‚ç‚¹å°†è¢«åŒ…å«åœ¨è§¦å‘å™¨ä¸­
def trigger_top(rank_value, rank_id, trigger_size, number_id): # æ ¹æ®æ’åºå€¼é€‰æ‹© trigger_size å¤§å°çš„èŠ‚ç‚¹ï¼Œç”¨äºåç»­çš„åé—¨æ”»å‡»
    local_id = []
    if number_id < trigger_size:
        trigger_size = number_id
    for i in range(int(trigger_size)):
        local_id.append(rank_id[i,0].tolist())
    return local_id

def trigger_top_c(rank_value, rank_id): # trigger_top çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œç»“åˆäº† detection1 å’Œ detection æ–¹æ³•æ¥è‡ªåŠ¨ç¡®å®š trigger_size çš„å¤§å°
    k = detection1(rank_value) # åˆ©ç”¨Gapå’ŒKmeansç¡®å®šé€‰æ‹©èšç±»æ•°é‡
    if k == 1:
        trigger_size = 3 # é€‰æ‹© 3 ä¸ªèŠ‚ç‚¹ä½œä¸ºè§¦å‘å™¨
    else:
        trigger_size = detection(rank_value, k) # è·å–ä¸€ä¸ªåŠ¨æ€çš„ trigger_sizeã€‚detection æ–¹æ³•ä¼šæ ¹æ®èšç±»çš„ç»“æœç¡®å®šé€‰æ‹©å¤šå°‘ä¸ªèŠ‚ç‚¹ä½œä¸ºè§¦å‘å™¨
        if trigger_size > 5:
            trigger_size = 5
        elif trigger_size < 3:
            trigger_size = 3
    
    local_id = []
    for i in range(trigger_size):
        local_id.append(rank_id[i,0].tolist())
    return local_id

class GradWhere(torch.autograd.Function): # äºŒå€¼åŒ–å¤„ç†ï¼Œç”¨äºåœ¨å›¾ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ä¸­åº”ç”¨ç‰¹å®šçš„æ¢¯åº¦æ“ä½œã€‚ï¼ˆ
    """
    We can implement our own custom autograd Functions by subclassing
    torch.autograd.Function and implementing the forward and backward passes
    which operate on Tensors.
    """

    @staticmethod
    def forward(ctx, input, thrd, device):
        """
        In the forward pass we receive a Tensor containing the input and return
        a Tensor containing the output. ctx is a context object that can be used
        to stash information for backward computation. You can cache arbitrary
        objects for use in the backward pass using the ctx.save_for_backward method.
        """
        ctx.save_for_backward(input)
        rst = torch.where(input>=thrd, torch.tensor(1.0, device=device, requires_grad=True),   #  å®ç°äº†åŸºäºé˜ˆå€¼çš„æ¡ä»¶æ“ä½œï¼Œå®ƒåŸºäºæ¡ä»¶åˆ¤æ–­è¾“å…¥å€¼æ˜¯å¦å¤§äºæŸä¸ªé˜ˆå€¼ thrdï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¾“å‡º 1ï¼Œå¦åˆ™è¾“å‡º 0ã€‚
                                      torch.tensor(0.0, device=device, requires_grad=True))
        return rst

    @staticmethod
    def backward(ctx, grad_output): #  GradWhere ä¸­çš„åå‘ä¼ æ’­ä»…ä»…æ˜¯ä¼ é€’ä»ä¸Šä¸€å±‚æ¥æ”¶åˆ°çš„æ¢¯åº¦ï¼ˆgrad_outputï¼‰ï¼Œå¹¶æ ¹æ®å‰å‘ä¼ æ’­çš„æ¡ä»¶å†³å®šæ¢¯åº¦çš„å€¼ã€‚
        """
        In the backward pass we receive a Tensor containing the gradient of the loss
        with respect to the output, and we need to compute the gradient of the loss
        with respect to the input.
        """
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        
        """
        Return results number should corresponding with .forward inputs (besides ctx),
        for each input, return a corresponding backward grad
        """
        return grad_input, None, None


'''
bkd_gids_train æ˜¯ä¸€ä¸ªåŒ…å«å›¾ ID çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢«é€‰æ‹©è¿›è¡Œåé—¨æ”»å‡»çš„å›¾ã€‚æ¯ä¸ªå›¾é€šè¿‡ bkd_gids_train æ ‡è¯†ï¼Œæ¶æ„å®¢æˆ·ç«¯å°†åœ¨è¿™äº›å›¾ä¸­æ³¨å…¥åé—¨è§¦å‘å™¨
bkd_nid_groups æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºå›¾ IDï¼Œå€¼ä¸ºä¸€ä¸ªåŒ…å«èŠ‚ç‚¹ ID çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºåœ¨å¯¹åº”å›¾ä¸­éœ€è¦è¿›è¡Œåé—¨æ”»å‡»çš„èŠ‚ç‚¹ç»„ã€‚æ¯ä¸ªå›¾ä¸­æœ‰ä¸€ç»„ç‰¹å®šçš„èŠ‚ç‚¹è¢«é€‰ä¸­ä½œä¸ºåé—¨æ³¨å…¥çš„ç›®æ ‡èŠ‚ç‚¹ã€‚
bkd_gids ä¹Ÿæ˜¯ä¸€ä¸ªåŒ…å«å›¾ ID çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºéœ€è¦è¿›è¡Œåé—¨æ”»å‡»çš„å›¾çš„é›†åˆã€‚ä¸ bkd_gids_train ç›¸ä¼¼ï¼Œä½†å…¶ä½œç”¨èŒƒå›´é€šå¸¸æ›´å¹¿ï¼Œå¯èƒ½æ¶‰åŠå¤šä¸ªé˜¶æ®µæˆ–ä¸åŒçš„æ•°æ®é›†
'''
class Generator(nn.Module): #  ç”Ÿæˆåé—¨æ”»å‡»å›¾
    def __init__(self, sq_dim, feat_dim, layernum, trigger_size, dropout=0.05):
        super(Generator, self).__init__()
        layers = []
        layers_feat = []
        view = []
        view_feat = []
        if dropout > 0: # å®šä¹‰äº†å¤„ç† ç»“æ„æ•°æ®ï¼ˆä¾‹å¦‚å›¾çš„é‚»æ¥çŸ©é˜µç­‰ï¼‰ çš„ç¥ç»ç½‘ç»œå±‚
            layers.append(nn.Dropout(p=dropout))
        for l in range(layernum-1):
            layers.append(nn.Linear(sq_dim, sq_dim))
            layers.append(nn.ReLU(inplace=True))
            if dropout > 0:
                layers.append(nn.Dropout(p=dropout))
        layers.append(nn.Linear(sq_dim, sq_dim))
        
        if dropout > 0: # å¤„ç† èŠ‚ç‚¹ç‰¹å¾æ•°æ® çš„ç½‘ç»œå±‚
            layers_feat.append(nn.Dropout(p=dropout))
        for l in range(layernum-1):
            layers_feat.append(nn.Linear(feat_dim, feat_dim))
            layers_feat.append(nn.ReLU(inplace=True))
            if dropout > 0:
                layers_feat.append(nn.Dropout(p=dropout))
        layers_feat.append(nn.Linear(feat_dim, feat_dim))

        if dropout > 0: # ç”¨äºå¤„ç† å›¾ç»“æ„æ•°æ® çš„ç½‘ç»œå±‚ï¼Œä¸ layers éƒ¨åˆ†ç±»ä¼¼ï¼Œä½†å®ƒç”¨äºå›¾çš„è¡¨ç¤ºå­¦ä¹ ã€‚
            view.append(nn.Dropout(p=dropout))
        for l in range(layernum-1):
            view.append(nn.Linear(sq_dim, sq_dim))
            view.append(nn.ReLU(inplace=True))
            if dropout > 0:
                view.append(nn.Dropout(p=dropout))
        view.append(nn.Linear(sq_dim, sq_dim))

        if dropout > 0: # ç”¨äºå¤„ç† ç‰¹å¾æ•°æ®çš„è¡¨ç¤ºå­¦ä¹ ï¼Œä¸ layers_feat éƒ¨åˆ†ç±»ä¼¼ï¼Œç”¨äºå­¦ä¹ ç‰¹å¾æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚
            view_feat.append(nn.Dropout(p=dropout))
        for l in range(layernum-1):
            view_feat.append(nn.Linear(feat_dim, feat_dim))
            view_feat.append(nn.ReLU(inplace=True))
            if dropout > 0:
                view_feat.append(nn.Dropout(p=dropout))
        view_feat.append(nn.Linear(feat_dim, feat_dim))
        
        self.sq_dim = sq_dim
        self.feat_dim = feat_dim
        self.trigger_size = trigger_size
        self.layers = nn.Sequential(*layers)
        self.layers_feat = nn.Sequential(*layers_feat)
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.avg_pool_feat = nn.AdaptiveAvgPool1d(1)
        self.mlp = nn.Linear(1, sq_dim*sq_dim)
        self.mlp_feat = nn.Linear(1, sq_dim*feat_dim)
        self.view = nn.Sequential(*view)
        self.view_feat = nn.Sequential(*view_feat)
        #self.mlp_pool = nn.AdaptiveAvgPool1d(1)
               
    def forward(self, args, id, graphs_train, bkd_gids_train, Ainput, Xinput, nodenums_id, 
                nodemax, is_Customized , is_test , trigger_size , device=torch.device('cpu'), binaryfeat=False):
        # Ainput:å›¾çš„æ‹“æ‰‘ç‰¹å¾çŸ©é˜µï¼ˆé‚»æ¥çŸ©é˜µï¼‰ã€‚Xinputï¼šå›¾çš„èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µã€‚
        # bkd_gids_trainï¼šéœ€è¦è¿›è¡Œåé—¨å¤„ç†çš„è®­ç»ƒå›¾çš„ ID é›†åˆã€‚

        bkd_nid_groups = {} # å­˜å‚¨æ¯ä¸ªå›¾ä¸­\éœ€è¦æ³¨å…¥åé—¨æ”»å‡»çš„èŠ‚ç‚¹ã€‚
        GW = GradWhere.apply # è‡ªå®šä¹‰çš„æ¢¯åº¦æ“ä½œå‡½æ•°ï¼Œåº”ç”¨äºåé—¨æ”»å‡»å›¾çš„è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚GradWhere ç±»ä¼šåœ¨å›¾çš„æ¢¯åº¦ä¼ æ’­ä¸­ä½¿ç”¨ã€‚

        graphs = copy.deepcopy(graphs_train)
        nodes_len = 0
        for gid in bkd_gids_train:#tqdm(bkd_gids_train):
            rst_bkdA_backbone = self.view(Ainput[gid]) # å¤„ç†å›¾é‚»æ¥çŸ©é˜µï¼Œä» Ainputï¼ˆé‚»æ¥çŸ©é˜µï¼‰ä¸­è·å–å›¾çš„ç»“æ„æ•°æ®ï¼Œå¹¶ä¼ å…¥ç½‘ç»œä¸­çš„ view å±‚
            if args.topo_activation=='relu':
                rst_bkdA_backbone = F.relu(rst_bkdA_backbone) # è®¾ç½®çš„æ¿€æ´»å‡½æ•°ç±»å‹ï¼ˆå¦‚ ReLU æˆ– Sigmoidï¼‰å¯¹è¾“å‡ºè¿›è¡Œæ¿€æ´»ã€‚
            elif args.topo_activation=='sigmoid':
                rst_bkdA_backbone = torch.sigmoid(rst_bkdA_backbone)    # nn.Functional.sigmoid is deprecated 28*28
            rst_bkdA_backbone = self.avg_pool(rst_bkdA_backbone)   # avg_pool å±‚è¿›è¡Œæ± åŒ–æ“ä½œã€‚æ± åŒ–æ“ä½œæœ‰åŠ©äºé™ç»´ï¼Œé€šå¸¸ç”¨äºåœ¨ç¥ç»ç½‘ç»œä¸­å‡å°‘ä¿¡æ¯é‡ï¼Œä½¿å¾—ç‰¹å¾è¡¨ç¤ºæ›´åŠ ç´§å‡‘ã€‚ 28*1
            
            rst_bkdX_backbone = self.view_feat(Xinput[gid]) # å¤„ç†èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µï¼Œå¯¹æ¯ä¸ªè®­ç»ƒå›¾ä¸­çš„èŠ‚ç‚¹ç‰¹å¾ Xinput è¿›è¡Œå¤„ç†ï¼Œä½¿ç”¨ view_feat å±‚ï¼ˆå…¨è¿æ¥å±‚ï¼‰æ¥å¤„ç†èŠ‚ç‚¹ç‰¹å¾ã€‚
            if args.feat_activation=='relu':
                rst_bkdX_backbone = F.relu(rst_bkdX_backbone)
            elif args.feat_activation=='sigmoid':
                rst_bkdX_backbone = torch.sigmoid(rst_bkdX_backbone)     # 28*5
            rst_bkdX_backbone = self.avg_pool_feat(rst_bkdX_backbone) # 28*1

            #########èŠ‚ç‚¹é‡è¦æ€§å¾—åˆ†å­¦ä¹ ##########
            trigger_id = torch.mul(rst_bkdA_backbone[:nodenums_id[gid]], # èŠ‚ç‚¹é‡è¦æ€§å¾—åˆ†ï¼Œé€šè¿‡å…ƒç´ çº§çš„ä¹˜æ³•æ“ä½œ(åº”è¯¥å°±æ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼Œå¯ä»¥ç¡®å®šå“ªäº›èŠ‚ç‚¹åœ¨å›¾ä¸­æœ€ä¸ºå…³é”®ï¼Œè¿›è€Œå†³å®šå“ªäº›èŠ‚ç‚¹å°†ä½œä¸ºè§¦å‘å™¨çš„å€™é€‰èŠ‚ç‚¹ã€‚
                                 rst_bkdX_backbone[:nodenums_id[gid]])

            trigger_l = GW(trigger_id, torch.mean(trigger_id), device) # äºŒå€¼åŒ–å¤„ç†ï¼Œä½¿ç”¨ GW æ¥å¯¹ trigger_id è¿›è¡Œå¤„ç†ï¼Œtorch.mean(trigger_id) è®¡ç®— trigger_id çš„å¹³å‡å€¼ä½œä¸ºå‚è€ƒã€‚å…¶ä¸­ä»…ä¿ç•™é«˜äºæˆ–ç­‰äºå‡å€¼çš„èŠ‚ç‚¹è¯„åˆ†ã€‚
            rank_value, rank_id = torch.sort(trigger_id, dim=0, descending=True) # å¯¹ trigger_id è¿›è¡Œæ’åºï¼Œå¾—åˆ°èŠ‚ç‚¹çš„æ’å rank_value å’Œå¯¹åº”çš„èŠ‚ç‚¹ID rank_idã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†ç¡®å®šå“ªäº›èŠ‚ç‚¹æœ€é€‚åˆè¢«æ³¨å…¥åé—¨ã€‚

            #########è§¦å‘å™¨ä½ç½®ï¼ˆå¤§å°ï¼‰å­¦ä¹ ##########
            bkd_nid_groups[gid] = trigger_top(rank_value, rank_id, self.trigger_size,nodenums_id[gid])  # é¢„å®šä¹‰ é€‰å–çš„åé—¨æ”»å‡»èŠ‚ç‚¹ï¼Œæ ¹æ® rank_value å’Œ rank_idï¼Œé€‰æ‹© å‰trigger_size æ•°é‡çš„èŠ‚ç‚¹ä½œä¸ºåé—¨æ”»å‡»çš„ç›®æ ‡èŠ‚ç‚¹ã€‚trigger_top é€‰å–æ’åå‰ trigger_size çš„èŠ‚ç‚¹ã€‚


       #######è§¦å‘å™¨å½¢çŠ¶å­¦ä¹ #########
        init_dr = init_trigger(
                        args, graphs, bkd_gids_train, bkd_nid_groups, 0.0) # æŠŠ bkd_nid_groups[gid] çš„èŠ‚ç‚¹ä¹‹é—´è¾¹åˆ é™¤ï¼Œå†è·å–é‚»æ¥çŸ©é˜µï¼Œæ›´æ”¹æ ‡ç­¾ï¼Œè·å–åº¦çŸ©é˜µæ·»åŠ åˆ°å›¾ä¸­å±æ€§ã€‚åˆå§‹åŒ–ç”Ÿæˆå¸¦æœ‰åé—¨æ”»å‡»çš„å›¾æ•°æ® ï¼Œé€šè¿‡ init_trigger å‡½æ•°åˆå§‹åŒ–åé—¨å›¾ï¼Œå¹¶ä¼šåœ¨å›¾ä¸­æ’å…¥åé—¨è§¦å‘å™¨ï¼ˆtriggerï¼‰
        bkd_dr = copy.deepcopy(init_dr)
        topomask, featmask = gen_mask(
                        graphs[0].node_features.shape[1], nodemax, bkd_dr, bkd_gids_train, bkd_nid_groups)
        # ç”Ÿæˆä¸åé—¨æ”»å‡»ç›¸å…³çš„æ‹“æ‰‘æ©ç ï¼ˆtopomaskä¸é‚»æ¥çŸ©é˜µç›¸å…³ï¼‰å’Œç‰¹å¾æ©ç ï¼ˆfeatmaskä¸èŠ‚ç‚¹ç‰¹å¾ç›¸å…³ï¼‰ã€‚
        # topomask ç”¨äºæ§åˆ¶å›¾ä¸­å“ªäº›è¾¹åœ¨åé—¨æ”»å‡»è¿‡ç¨‹ä¸­åº”è¯¥è¢«ä¿®æ”¹ï¼Œfeatmask ç”¨äºæ§åˆ¶å›¾ä¸­å“ªäº›èŠ‚ç‚¹ç‰¹å¾åœ¨åé—¨æ”»å‡»è¿‡ç¨‹ä¸­åº”è¯¥è¢«ä¿®æ”¹

        Ainput_trigger, Xinput_trigger = gen_input(init_dr, bkd_gids_train, nodemax) #ç”Ÿæˆåé—¨å›¾çš„è¾“å…¥æ•°æ®ï¼ˆé‚»æ¥çŸ©é˜µ Ainput_trigger å’ŒèŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ Xinput_triggerï¼‰ï¼Œç”¨äºè®­ç»ƒåé—¨æ”»å‡»æ¨¡å‹ã€‚

        id = torch.as_tensor(float(id)).unsqueeze(0)
        id_output = self.mlp(id) # é€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆMLPï¼‰ç”Ÿæˆä¸€ä¸ª ID è¾“å‡º id_outputï¼Œå®ƒå°†åé—¨æ”»å‡»çš„æ ‡è¯†ç¬¦ id æ˜ å°„åˆ°ä¸€ä¸ªé«˜ç»´ç©ºé—´ã€‚
        id_output = id_output.reshape(self.sq_dim,self.sq_dim)

        id_output_feat = self.mlp_feat(id) # é€šè¿‡å¦ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆmlp_featï¼‰å¤„ç† idï¼Œç”Ÿæˆä¸ç‰¹å¾ç›¸å…³çš„è¾“å‡º id_output_featï¼Œç”¨äºåé—¨å›¾çš„ç‰¹å¾æ“ä½œã€‚
        id_output_feat = id_output_feat.reshape(self.sq_dim,self.feat_dim)


        for gid in bkd_gids_train:
            Ainput_trigger[gid] = Ainput_trigger[gid] * id_output # å®¢æˆ·ç«¯çš„åµŒå…¥ä¿¡æ¯ä¸çŸ©é˜µç»“åˆï¼ŒAinput_trigger[gid] æ˜¯å›¾çš„é‚»æ¥çŸ©é˜µã€‚é€šè¿‡ä¸ id_output ç›¸ä¹˜ï¼Œé‚»æ¥çŸ©é˜µè¢«åŠ æƒã€‚é€šè¿‡å¼•å…¥ id_output æ¥è°ƒæ•´å›¾ç»“æ„çš„æ‹“æ‰‘å…³ç³»
            # æ ¹æ®å›¾çš„é‚»æ¥çŸ©é˜µï¼ˆAi_Bï¼‰è®¡ç®—è§¦å‘å™¨çš„è¾¹æ³¨æ„åŠ›çŸ©é˜µï¼ˆEi_triï¼‰ã€‚è¿™ä¸ªè¿‡ç¨‹ç”¨äºå†³å®šå“ªäº›è¾¹åº”è¯¥å±äºè§¦å‘å™¨ã€‚
            rst_bkdA = self.layers(Ainput_trigger[gid]) # ä»£è¡¨äº†å›¾çš„ç»“æ„ä¿¡æ¯çš„å˜åŒ–ï¼Œå³å›¾çš„æ‹“æ‰‘ç»“æ„ç»è¿‡ç½‘ç»œçš„å˜æ¢
            if args.topo_activation=='relu':
                rst_bkdA = F.relu(rst_bkdA)
            elif args.topo_activation=='sigmoid': #ç”¨çš„è¿™ä¸ª
                rst_bkdA = torch.sigmoid(rst_bkdA)    # nn.Functional.sigmoid is deprecated

            for_whom='topo'
            if for_whom == 'topo':  
                rst_bkdA = torch.div(torch.add(rst_bkdA, rst_bkdA.transpose(0, 1)), 2.0)
                # å°† rst_bkdA ä¸å…¶è½¬ç½®ç›¸åŠ ï¼Œç„¶åé™¤ä»¥ 2ã€‚ä½¿å¾—å›¾çš„é‚»æ¥çŸ©é˜µæˆä¸ºå¯¹ç§°çŸ©é˜µï¼ˆå³æ— å‘å›¾ï¼‰ï¼Œç¡®ä¿æ¯ä¸€å¯¹èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»æ˜¯åŒå‘çš„
            if for_whom == 'topo' or (for_whom == 'feat' and binaryfeat):
                rst_bkdA = GW(rst_bkdA, args.topo_thrd, device) #   æ¯”0.5å¤§çš„è®¾ç½®ä¸º1ï¼›è¿›è¡Œ æ¢¯åº¦ å¤„ç†ï¼Œå¯èƒ½æ˜¯ä¸ºäº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ§åˆ¶æ‹“æ‰‘ç»“æ„çš„æ›´æ–°ã€‚
            rst_bkdA = torch.mul(rst_bkdA, topomask[gid])
            # è¿›ä¸€æ­¥é™åˆ¶ rst_bkdA ä¸­çš„æŸäº›å…ƒç´ ã€‚topomask ç”¨äºæ§åˆ¶å“ªäº›èŠ‚ç‚¹çš„è¾¹åº”è¯¥è¢«ä¿®æ”¹ï¼Œå“ªäº›ä¸åº”è¯¥ã€‚æ©ç çš„ä½œç”¨æ˜¯å¯¹é‚»æ¥çŸ©é˜µåº”ç”¨ä¸€ä¸ªé®ç½©ï¼Œåªä¿ç•™æŒ‡å®šèŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥ã€‚

            bkd_dr[gid].edge_mat = torch.add(init_dr[gid].edge_mat, rst_bkdA[:nodenums_id[gid], :nodenums_id[gid]]) # æ›´æ–°åé—¨æ”»å‡»å›¾çš„é‚»æ¥çŸ©é˜µ
            for i in range(nodenums_id[gid]): # äºŒå€¼åŒ–å¤„ç†ï¼šä¸ºäº†ç¡®å®šèŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥çŠ¶æ€ï¼Œè¾¹çš„æ³¨æ„åŠ›çŸ©é˜µè¢«è½¬æ¢ä¸ºäºŒå€¼çŸ©é˜µ.æ·»åŠ è¾¹åˆ°å›¾ä¸­
                for j in range(nodenums_id[gid]):
                    if rst_bkdA[i][j] == 1 and i < j:
                        bkd_dr[gid].g.add_edge(i, j)
            bkd_dr[gid].node_tags = list(dict(bkd_dr[gid].g.degree).values()) #  æ›´æ–°èŠ‚ç‚¹æ ‡ç­¾ï¼Œé€šè¿‡è®¡ç®—å›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„åº¦ï¼ˆå³æ¯ä¸ªèŠ‚ç‚¹çš„è¿æ¥æ•°ï¼‰å¹¶å°†å…¶ä½œä¸ºèŠ‚ç‚¹æ ‡ç­¾å­˜å‚¨åœ¨ node_tags ä¸­ã€‚
         
            for_whom='feat'
            Xinput_trigger[gid] = Xinput_trigger[gid]*id_output_feat # å®¢æˆ·ç«¯çš„åµŒå…¥ä¿¡æ¯ä¸æ³¨æ„åŠ›çŸ©é˜µç»“åˆï¼Œå¤„ç†èŠ‚ç‚¹ç‰¹å¾
            rst_bkdX = self.layers_feat(Xinput_trigger[gid]) # é€šè¿‡å…¨è¿æ¥å±‚å¤„ç†èŠ‚ç‚¹ç‰¹å¾
            if args.feat_activation=='relu': # ç”¨çš„è¿™ä¸ª
                rst_bkdX = F.relu(rst_bkdX)
            elif args.feat_activation=='sigmoid':
                rst_bkdX = torch.sigmoid(rst_bkdX)
                
            if for_whom == 'topo': # not consider direct yet
                rst_bkdX = torch.div(torch.add(rst_bkdX, rst_bkdX.transpose(0, 1)), 2.0)
            # binaryfeat = True
            if for_whom == 'topo' or (for_whom == 'feat' and binaryfeat):
                rst_bkdX = GW(rst_bkdX, args.feat_thrd, device)
            rst_bkdX = torch.mul(rst_bkdX, featmask[gid])
            
            bkd_dr[gid].node_features = torch.add( 
                    rst_bkdX[:nodenums_id[gid]].detach().cpu(), torch.Tensor(init_dr[gid].node_features)) # æ›´æ–°èŠ‚ç‚¹ç‰¹å¾
            
        edges_len_avg = 0
        return bkd_dr, bkd_nid_groups, edges_len_avg, self.trigger_size, trigger_id, trigger_l
        # å›å¤„ç†åçš„åé—¨å›¾æ•°æ® bkd_drã€èŠ‚ç‚¹ç»„ bkd_nid_groupsã€å¹³å‡è¾¹é•¿ edges_len_avg ä»¥åŠå…¶ä»–ä¿¡æ¯ï¼ˆå¦‚è§¦å‘å™¨ ID å’Œè§¦å‘å™¨æ ‡ç­¾ trigger_id å’Œ trigger_lï¼‰
    
def SendtoCUDA(gid, items):
    """
    - items: a list of dict / full-graphs list, 
             used as item[gid] in items
    - gid: int
    """
    cuda = torch.device('cuda')
    for item in items:
        item[gid] = torch.as_tensor(item[gid], dtype=torch.float32).to(cuda)
        
        
def SendtoCPU(gid, items):
    """
    Used after SendtoCUDA, target object must be torch.tensor and already in cuda.
    
    - items: a list of dict / full-graphs list, 
             used as item[gid] in items
    - gid: int
    """
    
    cpu = torch.device('cpu')
    for item in items:
        item[gid] = item[gid].to(cpu)